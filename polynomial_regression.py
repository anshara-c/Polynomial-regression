# -*- coding: utf-8 -*-
"""polynomial_regression

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1R-iweStZlIwAW93lIOSnb-W4MsJgMuLD

<a href="https://colab.research.google.com/github/mirsazzathossain/CSE317-Lab/blob/autumn_2022/Lab_Assignment_04.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>
"""

import os
import numpy as np
import pandas as pd
from io import StringIO
from google.colab import drive
from PIL import Image
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
from sklearn.neighbors import KNeighborsClassifier

myData = pd.read_csv("data.csv", header=None)
myData.columns = ["Feature", "Label"]

features = myData["Feature"]
label = myData["Label"]

oneMatrix = np.ones((20, 1))
X = np.vstack(features.to_numpy())
X = np.append(oneMatrix, X, axis=1)
Y = np.vstack(label.to_numpy())

train_X, test_X = np.split(X, [int(0.75 * len(X))], axis = 0)
train_Y, test_Y = np.split(Y, [int(0.75 * len(Y))], axis = 0)

R = np.matmul(np.transpose(train_X), train_X)
R_INV = np.linalg.inv(R)
Q = train_X
Q_T = np.transpose(Q)
beta1 = np.matmul(R_INV, Q_T).dot(train_Y)

R = np.matmul(np.transpose(test_X), test_X)
R_INV = np.linalg.inv(R)
Q = test_X
Q_T = np.transpose(Q)
beta2 = np.matmul(R_INV, Q_T).dot(test_Y)

print(beta1)
print(beta2)

MSE_Linear = round(np.square(np.subtract(beta1,beta2)).mean(), 3)

print("{0}%".format(MSE_Linear))

"""#### **Polynomial Regression**

In this assignment, you will implement polynomial regression and apply it to the [Assignment 4 Dataset](https://minhaskamal.github.io/DownGit/#/home?url=https://github.com/mirsazzathossain/CSE317-Lab-Numerical-Methods/blob/main/datasets/data.csv).

The dataset contains two columns, the first column is the feature and the second column is the label. The goal is find the best fit line for the data.

You will need to perform the following regression tasks and find the best one for the dataset.

1.    **Linear Regression:**

     The equation we are trying to fit is:
     $$y = \theta_0 + \theta_1 x$$
     where $x$ is the feature and $y$ is the label.

     We can rewrite the equation in vector form as:
$$Y = X\theta$$ where $X$ is a matrix with two columns, the first column is all 1s and the second column is the feature, and $Y$ is a vector with the labels. $\theta$ is a vector with two elements, $\theta_0$ and $\theta_1$. The $X$ matrix will look like this:
$$X = \begin{bmatrix} 1 & x_1 \\ 1 & x_2 \\ \vdots & \vdots \\ 1 & x_n \end{bmatrix}$$
2. **Quadratic Regression:**

     The equation we are trying to fit is:
     $$y = \theta_0 + \theta_1 x + \theta_2 x^2$$
     where $x$ is the feature and $y$ is the label.

     We can rewrite the equation in vector form as:
$$Y = X\theta$$where $X$ is a matrix with three columns, the first column is all 1s, the second column is the feature, and the third column is the feature squared, and $Y$ is a vector with the labels. $\theta$ is a vector with three elements, $\theta_0$, $\theta_1$, and $\theta_2$. The $X$ matrix will look like this:

$$X = \begin{bmatrix} 1 & x_1 & x_1^2 \\ 1 & x_2 & x_2^2 \\ \vdots & \vdots & \vdots \\ 1 & x_n & x_n^2 \end{bmatrix}$$
3. **Cubic Regression:**

     The equation we are trying to fit is:
$$y = \theta_0 + \theta_1 x + \theta_2 x^2 + \theta_3 x^3$$
     where $x$ is the feature and $y$ is the label.

     We can rewrite the equation in vector form as:
$$Y = X\theta$$where $X$ is a matrix with four columns, the first column is all 1s, the second column is the feature, the third column is the feature squared, and the fourth column is the feature cubed, and $Y$ is a vector with the labels. $\theta$ is a vector with four elements, $\theta_0$, $\theta_1$, $\theta_2$, and $\theta_3$. The $X$ matrix will look like this:
$$X = \begin{bmatrix} 1 & x_1 & x_1^2 & x_1^3 \\ 1 & x_2 & x_2^2 & x_2^3 \\ \vdots & \vdots & \vdots & \vdots \\ 1 & x_n & x_n^2 & x_n^3 \end{bmatrix}$$

Take 15 data points from the dataset and use them as the training set. Use the remaining data points as the test set. For each regression task, find the best $\theta$ vector using the training set. Then, calculate the mean squared error (MSE) on the test set. Plot the training set, the test set (in a different color), and the best fit line for each regression task. Which regression task gives the best fit line? Which regression task gives the lowest MSE on the test set? Report your answers in a Markdown cell.

**Note:** Do not use any built-in functions like `np.polyfit` or `sklearn.linear_model.LinearRegression` or any other built-in functions that perform polynomial regression. You must implement the regression tasks yourself.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

data = pd.read_csv('data.csv', header=None)
data = data.rename(columns={0:"Feature", 1:"Label"})
print(type(data))

print(data.head())

Feature = data['Feature']
Label = data['Label']

Feature = Feature.to_numpy()
Label = Label.to_numpy()

ones = np.ones(shape=len(Feature))

A = np.c_[ones, Feature]
print(A)

plt.scatter(a, b)
plt.xlabel('pages')
plt.ylabel('price')
plt.show()

n = size(A,2);
out = A*ones(n,1)/n;
avg_Label = np.dot(np.ones(Label.size), Label)/Label.size

a_tilde = A - avg_A
Label_tilde = Label - avg_Label

std_A = np.sqrt(np.dot(A - avg_A), (A - avg_A))/(A.size-1)
std_Label = np.sqrt(np.dot((Label - avg_Label), (Label - avg_Label))/(Label.size-1))

rho = np.dot(a_tilde, Label_tilde)/(np.sqrt(np.dot(a_tilde, a_tilde))*np.sqrt(np.dot(Label_tilde, Label_tilde)))

c_2 = (rho*std_Label)/std_A
c_1 = avg_Label - avg_A*c_2

print(c_1, c_2)